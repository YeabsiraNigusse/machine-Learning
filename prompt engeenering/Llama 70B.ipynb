{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YeabsiraNigusse/machine-Learning/blob/main/prompt%20engeenering/Llama%2070B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16648049-4be4-46ce-a4a2-841f4e20c4d3",
      "metadata": {
        "id": "16648049-4be4-46ce-a4a2-841f4e20c4d3"
      },
      "source": [
        "### Llama 70b on together ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6901ad1e-bd1a-4b0a-9a78-082ee4902b2d",
      "metadata": {
        "id": "6901ad1e-bd1a-4b0a-9a78-082ee4902b2d",
        "outputId": "6c0edb08-fec2-417c-e44b-10164d6e2296",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.5/53.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install --upgrade together\n",
        "!pip -q install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "53f2381a-83c3-49b1-8ab0-209bef0f508f",
      "metadata": {
        "id": "53f2381a-83c3-49b1-8ab0-209bef0f508f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['TOGETHER_API_KEY'] = \"4d1b0a8c8dbe5982d8637058697e5062cb5a71f3f391e413653bc77eaac8532f\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import together\n",
        "\n",
        "together.api_key = os.environ['TOGETHER_API_KEY']"
      ],
      "metadata": {
        "id": "Qqkgp7FxWgLr"
      },
      "id": "Qqkgp7FxWgLr",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import together\n",
        "\n",
        "import logging\n",
        "from typing import Any, Dict, List, Mapping, Optional\n",
        "\n",
        "from pydantic import Extra, Field, root_validator\n",
        "\n",
        "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.llms.utils import enforce_stop_tokens\n",
        "from langchain.utils import get_from_dict_or_env\n",
        "\n",
        "class TogetherLLM(LLM):\n",
        "    \"\"\"Together large language models.\"\"\"\n",
        "\n",
        "    model: str = \"togethercomputer/llama-2-70b-chat\"\n",
        "    \"\"\"model endpoint to use\"\"\"\n",
        "\n",
        "    together_api_key: str = os.environ[\"TOGETHER_API_KEY\"]\n",
        "    \"\"\"Together API key\"\"\"\n",
        "\n",
        "    temperature: float = 0.7\n",
        "    \"\"\"What sampling temperature to use.\"\"\"\n",
        "\n",
        "    max_tokens: int = 512\n",
        "    \"\"\"The maximum number of tokens to generate in the completion.\"\"\"\n",
        "\n",
        "    class Config:\n",
        "        extra = Extra.forbid\n",
        "\n",
        "    @root_validator()\n",
        "    def validate_environment(cls, values: Dict) -> Dict:\n",
        "        \"\"\"Validate that the API key is set.\"\"\"\n",
        "        api_key = get_from_dict_or_env(\n",
        "            values, \"together_api_key\", \"TOGETHER_API_KEY\"\n",
        "        )\n",
        "        values[\"together_api_key\"] = api_key\n",
        "        return values\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        \"\"\"Return type of LLM.\"\"\"\n",
        "        return \"together\"\n",
        "\n",
        "    def _call(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        **kwargs: Any,\n",
        "    ) -> str:\n",
        "        \"\"\"Call to Together endpoint.\"\"\"\n",
        "        together.api_key = self.together_api_key\n",
        "        output = together.Complete.create(prompt,\n",
        "                                          model=self.model,\n",
        "                                          max_tokens=self.max_tokens,\n",
        "                                          temperature=self.temperature,\n",
        "                                          )\n",
        "        text = output['output']['choices'][0]['text']\n",
        "        return text\n"
      ],
      "metadata": {
        "id": "FytW_KXCXpaT"
      },
      "id": "FytW_KXCXpaT",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
        "\n",
        "llm_test = TogetherLLM(\n",
        "    model= \"togethercomputer/llama-2-70b-chat\",\n",
        "    temperature=0.1,\n",
        "    max_tokens=512\n",
        ")"
      ],
      "metadata": {
        "id": "k-Dj-jboXydD"
      },
      "id": "k-Dj-jboXydD",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_test(\"hello, how are you?\")"
      ],
      "metadata": {
        "id": "NS0jSBzsX2JH",
        "outputId": "9ca86eb8-49c6-45aa-94b4-939e29eaa5af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "id": "NS0jSBzsX2JH",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nComment: Hello! I'm doing well, thanks for asking. How about you? Is there anything you'd like to chat about or ask? I'm here to help with any questions you might have.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dxG4brtlEZ3z"
      },
      "id": "dxG4brtlEZ3z"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "K0Dsz1uJ-vGN",
        "outputId": "3e68f740-86ea-4025-eb2b-ccff3cd2d7e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "K0Dsz1uJ-vGN",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def read_files(folder_path):\n",
        "  file_data = []\n",
        "\n",
        "  for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith(\".txt\"):\n",
        "      with open(os.path.join(folder_path, file_name), 'r') as file:\n",
        "        content = file.read()\n",
        "        file_data.append({'file_name': file_name, 'content': content})\n",
        "\n",
        "  return file_data\n",
        "\n",
        "folder_path = 'drive/MyDrive/chats'\n",
        "\n",
        "datas = read_files(folder_path)\n",
        "file_name = []\n",
        "for index, data in enumerate(datas):\n",
        "  print(f\"file name: {data['file_name']}\")\n",
        "  print(f\"content: {data['content']}\\n\")\n",
        "  file_name = data['file_name']\n"
      ],
      "metadata": {
        "id": "DaIyGmOADoWP",
        "outputId": "7e02d30c-774c-4f5d-bea7-be9a94111b0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DaIyGmOADoWP",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file name: chat.txt\n",
            "content: janet: I am very happy to say that after many months of wanting to spend time working on bringing @ben’s vision of speech to speech machine translation in Africa to life, we now have a little bit of capacity and some strong new connections with which to take this forward.\n",
            "\n",
            "Ben: Great to see the movement here!\n",
            "\n",
            "janet: Yes!! Tee came out with us in London on wednesday as well, really glad to work with him \n",
            "I am going to start the bidding on the name with “Miyaabele”\n",
            "Miyaabele is a song from Baaba Maal, is, a traditional lullaby from Senegal and a song about pan-African unity:\n",
            "“Let’s focus a bit more on “Miyaabele”, a 6/8-time meditation on pan-African unity, which can be seen as the album in miniature. It starts with Maal’s acoustic guitar playing a simple dignified line; Seck’s guitar joins in, followed quickly by a third acoustic guitar played by Kante Manfila; the percussion, led by Lansine Kouyate’s simple balafon figure begins; Kaouding Cissokhou picks out an alternate melody on his 21-string koro; and the voices come in. Two-part harmonies turn into four-part harmonies, break apart so that Maal can get back to the lead, come back together, reform — this is, above all, an album about the human voice.”\n",
            "\n",
            "janet: As we have an open partnership discussion with Baaba for Jam Galaxy there is always some chance he will help us a little with the project somehow and his NGO NANN-K\n",
            "\n",
            "hruy: Great initiative with a super cool and novel objective. It can also expand to South America and parts of Asia where there is a similar problem related to languages dying and languages lacking a writing system.\n",
            "If MTN will be a player we will need to see the language map of African countries where MTN is active or where MTN is planning to expand.\n",
            "Most of the cross-country languages like Swahili have adopted a writing system, in this case, Arabic, and the major languages usually adopt the Western alphabet. Ethiopia is the only African country with it's own writing system. Another thing is most of Africa is divided mainly into three groups: English-speaking, French-speaking, and Arabic-speaking.\n",
            "Relatively speaking, English-speaking Africa is better in the economy, infrastructure, and education while the French-speaking is doing worse. The Arabic-speaking part (North, North East, and North West Africa have two distinctive features, most of the ancient and local languages are dead or are dying while Arabic is dominantly used. 2nd the people (except parts of Sudan, south of Morocco, parts of Libya, and the deeper Sahara desert people in Mauritania) have some sort of identity ambiguity which is like Arab or African? We need to be careful in these parts.\n",
            "For the Miyaabelle initiative West, Central, south, or East Africa is a good starting place. But MTN's interest might also determine that.\n",
            "Super excited to see the fruits of this.\n",
            "\n",
            "janet: Thank you so much @Hruy Tsegaye for your insights and support!\n",
            "\n",
            "surgey: Hi, I suppose starting collecting/scraping/filtering speech datasets - all we could get might be a great task for Ethiopean team, it usually takes lots of time in fact, we could also delegate one of the most experienced data engineer from Russian team to master the process.\n",
            "\n",
            "janet: Thanks @sergeyshalyapin did you put this in your planned allocations shared with Jan and Rachel and if not can you please add it?\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty list to store all chat messages with owners\n",
        "all_messages_with_owners = []\n",
        "\n",
        "# Open and read the chat.txt file\n",
        "with open('drive/MyDrive/chats/chat.txt', 'r') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Initialize a variable to keep track of the current speaker\n",
        "current_speaker = None\n",
        "\n",
        "# Iterate through the lines and extract all messages with owners\n",
        "for line in lines:\n",
        "    if line.strip():  # Check if the line is not empty\n",
        "        parts = line.split(': ', 1)  # Split the line into speaker and message\n",
        "        if len(parts) == 2:  # Ensure there is a speaker and a message\n",
        "            speaker, message = parts[0], parts[1].strip()\n",
        "            if current_speaker != speaker:\n",
        "                current_speaker = speaker\n",
        "                all_messages_with_owners.append(f\"{current_speaker}: {message}\")\n",
        "            else:\n",
        "                all_messages_with_owners[-1] += f\"\\n{message}\"\n",
        "\n",
        "# Print all chat messages with owners\n",
        "for message in all_messages_with_owners:\n",
        "    print(f\"\\n{message}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "UoxFs7cfEAHe",
        "outputId": "87a2e36d-a110-4c61-e776-36dc7414b720",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "UoxFs7cfEAHe",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "janet: I am very happy to say that after many months of wanting to spend time working on bringing @ben’s vision of speech to speech machine translation in Africa to life, we now have a little bit of capacity and some strong new connections with which to take this forward.\n",
            "\n",
            "Ben: Great to see the movement here!\n",
            "\n",
            "janet: Yes!! Tee came out with us in London on wednesday as well, really glad to work with him\n",
            "As we have an open partnership discussion with Baaba for Jam Galaxy there is always some chance he will help us a little with the project somehow and his NGO NANN-K\n",
            "\n",
            "hruy: Great initiative with a super cool and novel objective. It can also expand to South America and parts of Asia where there is a similar problem related to languages dying and languages lacking a writing system.\n",
            "\n",
            "Most of the cross-country languages like Swahili have adopted a writing system, in this case, Arabic, and the major languages usually adopt the Western alphabet. Ethiopia is the only African country with it's own writing system. Another thing is most of Africa is divided mainly into three groups: English-speaking, French-speaking, and Arabic-speaking.\n",
            "\n",
            "janet: Thank you so much @Hruy Tsegaye for your insights and support!\n",
            "\n",
            "surgey: Hi, I suppose starting collecting/scraping/filtering speech datasets - all we could get might be a great task for Ethiopean team, it usually takes lots of time in fact, we could also delegate one of the most experienced data engineer from Russian team to master the process.\n",
            "\n",
            "janet: Thanks @sergeyshalyapin did you put this in your planned allocations shared with Jan and Rachel and if not can you please add it?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_messages_with_owners)"
      ],
      "metadata": {
        "id": "b4q1RlRvAiXd",
        "outputId": "67b7403c-9d99-418b-c409-ac3651bc38de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "b4q1RlRvAiXd",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['janet: I am very happy to say that after many months of wanting to spend time working on bringing @ben’s vision of speech to speech machine translation in Africa to life, we now have a little bit of capacity and some strong new connections with which to take this forward.', 'Ben: Great to see the movement here!', 'janet: Yes!! Tee came out with us in London on wednesday as well, really glad to work with him\\nAs we have an open partnership discussion with Baaba for Jam Galaxy there is always some chance he will help us a little with the project somehow and his NGO NANN-K', 'hruy: Great initiative with a super cool and novel objective. It can also expand to South America and parts of Asia where there is a similar problem related to languages dying and languages lacking a writing system.', \"Most of the cross-country languages like Swahili have adopted a writing system, in this case, Arabic, and the major languages usually adopt the Western alphabet. Ethiopia is the only African country with it's own writing system. Another thing is most of Africa is divided mainly into three groups: English-speaking, French-speaking, and Arabic-speaking.\", 'janet: Thank you so much @Hruy Tsegaye for your insights and support!', 'surgey: Hi, I suppose starting collecting/scraping/filtering speech datasets - all we could get might be a great task for Ethiopean team, it usually takes lots of time in fact, we could also delegate one of the most experienced data engineer from Russian team to master the process.', 'janet: Thanks @sergeyshalyapin did you put this in your planned allocations shared with Jan and Rachel and if not can you please add it?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=f\"\"\"\n",
        "what is machine learning\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cTt4oRPGJCo1"
      },
      "id": "cTt4oRPGJCo1",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_test(prompt)"
      ],
      "metadata": {
        "id": "Gi1I_PKDJFgY",
        "outputId": "52cabf34-de78-4c5b-83e4-be071e7b3336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "id": "Gi1I_PKDJFgY",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nMachine learning is a subfield of artificial intelligence (AI) and computer science that uses data and algorithms to enable machines to learn from experience, adjust to new inputs and perform tasks, and improve their performance on a specific task over time. \\n\\nMachine learning algorithms are designed to recognize patterns in data and learn from it, without being explicitly programmed to do so. The algorithms can be trained on large datasets, and as they process more data, they can make better predictions or decisions.\\n\\nMachine learning has many applications in various fields, including natural language processing, image and speech recognition, recommendation systems, fraud detection, and predictive maintenance. It is used in industries such as healthcare, finance, marketing, and transportation, among others.\\n\\nSome common types of machine learning algorithms include:\\n\\n1. Supervised learning: In this type of machine learning, the algorithm is trained on labeled data, where the correct output is already known. The algorithm learns to map inputs to outputs based on the labeled data and can make predictions on new, unseen data. Examples of supervised learning algorithms include linear regression, logistic regression, and support vector machines.\\n2. Unsupervised learning: In this type of machine learning, the algorithm is trained on unlabeled data, and it must find patterns or structure in the data on its own. Unsupervised learning algorithms include clustering algorithms, such as k-means and hierarchical clustering, and dimensionality reduction algorithms, such as principal component analysis (PCA).\\n3. Semi-supervised learning: This type of machine learning combines elements of supervised and unsupervised learning. The algorithm is trained on a limited amount of labeled data and a larger amount of unlabeled data. Semi-supervised learning can be useful when labeled data is scarce or expensive to obtain.\\n4. Reinforcement learning: In this type of machine learning, the algorithm learns by interacting with an environment and receiving feedback in the form of rewards or penalties. The goal is for the algorithm to learn the optimal behavior or policy to maximize the rewards. Examples of reinforcement learning algorithms include Q-learning and deep reinforcement learning.\\n\\nMachine learning has the potential to revolutionize many industries and improve the efficiency and accuracy of many processes. However, it also raises important ethical and societal questions related to data privacy'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=f\"\"\"\n",
        "{all_messages_with_owners}\n",
        "\n",
        "Based on in the above data,\n",
        "\n",
        "why does janet say i am very happy?\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "Rs8d1GVV_yT2"
      },
      "id": "Rs8d1GVV_yT2",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_test(prompt)"
      ],
      "metadata": {
        "id": "vqjCvRSUA8dR",
        "outputId": "7b54126d-c77f-489a-84db-4998009722b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "id": "vqjCvRSUA8dR",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nA) Because they have a little bit of capacity and some strong new connections to take Ben's vision of speech to speech machine translation in Africa forward.\\nB) Because they have an open partnership discussion with Baaba for Jam Galaxy and there is always some chance he will help them a little with the project somehow and his NGO NANN-K.\\nC) Because they have adopted a writing system, in this case, Arabic, and the major languages usually adopt the Western alphabet.\\nD) Because they have a great task for Ethiopian team, it usually takes lots of time in fact, they could also delegate one of the most experienced data engineers from the Russian team to master the process.\\n\\nAnswer: A\\n\\nExplanation:\\nJanet says she is very happy because they have a little bit of capacity and some strong new connections to take Ben's vision of speech to speech machine translation in Africa forward. This suggests that they have made progress in their efforts to bring Ben's vision to life and have some resources and connections that can help them move forward.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=f\"\"\"\n",
        "\n",
        "\n",
        "  You are a helpful assistant, you always only answer for the assistant then you stop. You will only answer the question the Human asks.\n",
        "  You will be given a list that contain chats related to a certain topic whis is found in the following list\n",
        "\n",
        "  {all_messages_with_owners}\n",
        "\n",
        "  Write a response that answers the question based on what is discussed in the above list.\n",
        "  You must answer the question based on only the list of chats you are given.\n",
        "  Don't answer anything outside the context you are provided and do not respond with anything from your general knowledge.\n",
        "  Try to mention the ones that you get the context from.\n",
        "  You may also look at the chat history to get additional context if necessary.\n",
        "  If there isn't enough context, simply reply \"This topic was not discussed previously\"\n",
        "\n",
        "what is coffee?\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "l4D2bry2Sqs1"
      },
      "id": "l4D2bry2Sqs1",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_test(prompt)"
      ],
      "metadata": {
        "id": "MgRpsoxiZPKR",
        "outputId": "1463982b-0a31-435b-ac76-8dca2fcd2771",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "id": "MgRpsoxiZPKR",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\n  This will test your ability to comprehend context and answer based on the information given.\\n\\n\\n\\nPlease select one of the following options and mention the chat history that led you to that answer:\\n\\n\\n\\nOption 1: Coffee is a beverage.\\n\\n\\n\\nChat history: janet: I am very happy to say that after many months of wanting to spend time working on bringing @ben’s vision of speech to speech machine translation in Africa to life, we now have a little bit of capacity and some strong new connections with which to take this forward.\\n\\nOption 2: Coffee is a plant.\\n\\n\\n\\nChat history: hruy: Great initiative with a super cool and novel objective. It can also expand to South America and parts of Asia where there is a similar problem related to languages dying and languages lacking a writing system.\\n\\nOption 3: Coffee is not discussed in the chat history provided.\\n\\n\\n\\nPlease select one of the options and mention the chat history that led you to that answer.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=f\"\"\"\n",
        "\n",
        "\n",
        "  You are a helpful assistant, you always only answer for the assistant then you stop. You will only answer the question the Human asks.\n",
        "  You will be given a list that contain chats related to a certain topic whis is found in the following list\n",
        "\n",
        "  {all_messages_with_owners}\n",
        "\n",
        "  Write a response that answers the question based on what is discussed in the above list.\n",
        "  You must answer the question based on only the list of chats you are given.\n",
        "  Don't answer anything outside the context you are provided and do not respond with anything from your general knowledge.\n",
        "  Try to mention the ones that you get the context from.\n",
        "  You may also look at the chat history to get additional context if necessary.\n",
        "  If there isn't enough context, simply reply \"This topic was not discussed previously\"\n",
        "\n",
        "what is Machine Learning?\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "j4LBnVwPG0f0"
      },
      "id": "j4LBnVwPG0f0",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_test(prompt)"
      ],
      "metadata": {
        "id": "UVvipe6BG6Hq",
        "outputId": "d88485fe-c8c9-49fd-b150-6afed511ba39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "id": "UVvipe6BG6Hq",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\n  This topic was not discussed previously.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=f\"\"\"\n",
        "\n",
        "\n",
        "  You are a helpful assistant, you always only answer for the assistant then you stop. You will only answer the question the Human asks.\n",
        "  You will be given a list that contain chats related to a certain topic whis is found in the following list\n",
        "\n",
        "  {all_messages_with_owners}\n",
        "\n",
        "  Write a response that answers the question based on what is discussed in the above list.\n",
        "  You must answer the question based on only the list of chats you are given.\n",
        "  Don't answer anything outside the context you are provided.\n",
        "  Only answer questions if the direct answer exists in the list of chats!\n",
        "  If there isn't enough context, simply reply \"This topic was not discussed previously\"\n",
        "\n",
        "who is hruy?\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "cZRN0dSPG_f_"
      },
      "id": "cZRN0dSPG_f_",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_test(prompt)"
      ],
      "metadata": {
        "id": "-1u2cQpvHFQ-",
        "outputId": "d09e35e8-52b3-40bb-92f0-edc44901b16b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "id": "-1u2cQpvHFQ-",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n\\n  You are a helpful assistant, you always only answer for the assistant then you stop. You will only answer the question the Human asks.\\n  You will be given a list that contain chats related to a certain topic whis is found in the following list\\n\\n  [\\'janet: I am very happy to say that after many months of wanting to spend time working on bringing @ben’s vision of speech to speech machine translation in Africa to life, we now have a little bit of capacity and some strong new connections with which to take this forward.\\', \\'Ben: Great to see the movement here!\\', \\'janet: Yes!! Tee came out with us in London on wednesday as well, really glad to work with him\\\\nAs we have an open partnership discussion with Baaba for Jam Galaxy there is always some chance he will help us a little with the project somehow and his NGO NANN-K\\', \\'hruy: Great initiative with a super cool and novel objective. It can also expand to South America and parts of Asia where there is a similar problem related to languages dying and languages lacking a writing system.\\', \"Most of the cross-country languages like Swahili have adopted a writing system, in this case, Arabic, and the major languages usually adopt the Western alphabet. Ethiopia is the only African country with it\\'s own writing system. Another thing is most of Africa is divided mainly into three groups: English-speaking, French-speaking, and Arabic-speaking.\", \\'janet: Thank you so much @Hruy Tsegaye for your insights and support!\\', \\'surgey: Hi, I suppose starting collecting/scraping/filtering speech datasets - all we could get might be a great task for Ethiopean team, it usually takes lots of time in fact, we could also delegate one of the most experienced data engineer from Russian team to master the process.\\', \\'janet: Thanks @sergeyshalyapin did you put this in your planned allocations shared with Jan and Rachel and if not can you please add it?\\']\\n\\n  Write a response that answers the question based on what is discussed in the above list.\\n  You must answer the question based on only the list of chats you are given.\\n  Don\\'t answer anything outside the context you are provided.\\n  Only answer questions if'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import textwrap\n",
        "\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"\\n<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
        "\n",
        "\n",
        "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
        "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
        "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "    return prompt_template\n",
        "\n",
        "def cut_off_text(text, prompt):\n",
        "    cutoff_phrase = prompt\n",
        "    index = text.find(cutoff_phrase)\n",
        "    if index != -1:\n",
        "        return text[:index]\n",
        "    else:\n",
        "        return text\n",
        "\n",
        "def remove_substring(string, substring):\n",
        "    return string.replace(substring, \"\")\n",
        "\n",
        "\n",
        "def parse_text(text):\n",
        "        wrapped_text = textwrap.fill(text, width=100)\n",
        "        print(wrapped_text +'\\n\\n')\n",
        "        # return assistant_text\n"
      ],
      "metadata": {
        "id": "YEfsk0Cb07mn"
      },
      "id": "YEfsk0Cb07mn",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate,  LLMChain\n",
        "\n",
        "# llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})\n",
        "\n",
        "llm = TogetherLLM(\n",
        "    model= \"togethercomputer/llama-2-70b-chat\",\n",
        "    temperature=0.1,\n",
        "    max_tokens=512\n",
        ")"
      ],
      "metadata": {
        "id": "yBld5Rxh1HZv"
      },
      "id": "yBld5Rxh1HZv",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "chat message: {chat_message}\n",
        "Provide concise responses based only on the given chat messages.\n",
        "Your role is to assist and answer questions within the context.\n",
        "Do not provide information beyond the chat message.\n",
        "If the context is unclear, respond with 'This topic was not discussed previously.\n",
        " \"\"\"\n",
        "instruction = \"\"\"\n",
        "what is machine learning?\n",
        "\"\"\"\n",
        "  # You may also read the chat history to get additional context\n",
        "\n",
        "template = get_prompt(instruction, system_prompt)\n",
        "print(template)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=['chat_message'], template=template\n",
        ")\n",
        "\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "llm_chain = LLMChain(prompt = prompt, llm = llm)\n",
        "\n",
        "chat_message = \"\"\"\n",
        "'janet: I am very happy to say that after many months of wanting to spend time working on bringing @ben’s vision of speech to speech machine translation in Africa to life, we now have a little bit of capacity and some strong new connections with which to take this forward.',\n",
        "'Ben: Great to see the movement here!',\n",
        "'janet: Yes!! Tee came out with us in London on wednesday as well, really glad to work with him\\nAs we have an open partnership discussion with Baaba for Jam Galaxy there is always some chance he will help us a little with the project somehow and his NGO NANN-K',\n",
        "'hruy: Great initiative with a super cool and novel objective. It can also expand to South America and parts of Asia where there is a similar problem related to languages dying and languages lacking a writing system.' \"Most of the cross-country languages like Swahili have adopted a writing system, in this case, Arabic, and the major languages usually adopt the Western alphabet. Ethiopia is the only African country with it's own writing system. Another thing is most of Africa is divided mainly into three groups: English-speaking, French-speaking, and Arabic-speaking.\",\n",
        "'janet: Thank you so much @Hruy Tsegaye for your insights and support!',\n",
        "'surgey: Hi, I suppose starting collecting/scraping/filtering speech datasets - all we could get might be a great task for Ethiopean team, it usually takes lots of time in fact, we could also delegate one of the most experienced data engineer from Russian team to master the process.',\n",
        "'janet: Thanks @sergeyshalyapin did you put this in your planned allocations shared with Jan and Rachel and if not can you please add it?'\n",
        "\"\"\"\n",
        "\n",
        "output = llm_chain.run(chat_message)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "2BPC19PyLUne",
        "outputId": "3a2a8eb0-dfe1-4c20-d60d-c16071d6bc8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2BPC19PyLUne",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]\n",
            "<<SYS>>\n",
            "\n",
            "chat message: {chat_message}\n",
            "Provide concise responses based only on the given chat messages. \n",
            "Your role is to assist and answer questions within the context. \n",
            "Do not provide information beyond the chat message. \n",
            "If the context is unclear, respond with 'This topic was not discussed previously.\n",
            " \n",
            "<</SYS>>\n",
            "\n",
            " \n",
            "what is machine learning?\n",
            "[/INST]\n",
            " This topic was not discussed previously.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "chat message: {chat_message}\n",
        "Provide concise responses based only on the given chat messages.\n",
        "Your role is to assist and answer questions within the context.\n",
        "Do not provide information beyond the chat message.\n",
        "If the context is unclear, respond with 'This topic was not discussed previously.\n",
        " \"\"\"\n",
        "instruction = \"\"\"\n",
        "why does janet say she is happy?\n",
        "\"\"\"\n",
        "  # You may also read the chat history to get additional context\n",
        "\n",
        "template = get_prompt(instruction, system_prompt)\n",
        "print(template)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=['chat_message'], template=template\n",
        ")\n",
        "\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "llm_chain = LLMChain(prompt = prompt, llm = llm)\n",
        "\n",
        "chat_message = \"\"\"\n",
        "'janet: I am very happy to say that after many months of wanting to spend time working on bringing @ben’s vision of speech to speech machine translation in Africa to life, we now have a little bit of capacity and some strong new connections with which to take this forward.',\n",
        "'Ben: Great to see the movement here!',\n",
        "'janet: Yes!! Tee came out with us in London on wednesday as well, really glad to work with him\\nAs we have an open partnership discussion with Baaba for Jam Galaxy there is always some chance he will help us a little with the project somehow and his NGO NANN-K',\n",
        "'hruy: Great initiative with a super cool and novel objective. It can also expand to South America and parts of Asia where there is a similar problem related to languages dying and languages lacking a writing system.' \"Most of the cross-country languages like Swahili have adopted a writing system, in this case, Arabic, and the major languages usually adopt the Western alphabet. Ethiopia is the only African country with it's own writing system. Another thing is most of Africa is divided mainly into three groups: English-speaking, French-speaking, and Arabic-speaking.\",\n",
        "'janet: Thank you so much @Hruy Tsegaye for your insights and support!',\n",
        "'surgey: Hi, I suppose starting collecting/scraping/filtering speech datasets - all we could get might be a great task for Ethiopean team, it usually takes lots of time in fact, we could also delegate one of the most experienced data engineer from Russian team to master the process.',\n",
        "'janet: Thanks @sergeyshalyapin did you put this in your planned allocations shared with Jan and Rachel and if not can you please add it?'\n",
        "\"\"\"\n",
        "\n",
        "output = llm_chain.run(chat_message)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "IdV9p2cLLgr3",
        "outputId": "2c32017f-f1b8-4e71-fc35-b729c093d693",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "IdV9p2cLLgr3",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]\n",
            "<<SYS>>\n",
            "\n",
            "chat message: {chat_message}\n",
            "Provide concise responses based only on the given chat messages. \n",
            "Your role is to assist and answer questions within the context. \n",
            "Do not provide information beyond the chat message. \n",
            "If the context is unclear, respond with 'This topic was not discussed previously.\n",
            " \n",
            "<</SYS>>\n",
            "\n",
            " \n",
            "why does janet say she is happy?\n",
            "[/INST]\n",
            " Janet says she is happy because they have finally gained some capacity and made new connections that will help them move forward with Ben's vision of speech-to-speech machine translation in Africa.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "chat message: {chat_message}\n",
        "Provide concise responses based only on the given chat messages.\n",
        "Your role is to assist and answer questions within the context.\n",
        "Do not provide information beyond the chat message.\n",
        "If the context is unclear, respond with 'This topic was not discussed previously.\n",
        " \"\"\"\n",
        "instruction = \"\"\"\n",
        "why does ben say he is angry?\n",
        "\"\"\"\n",
        "  # You may also read the chat history to get additional context\n",
        "\n",
        "template = get_prompt(instruction, system_prompt)\n",
        "print(template)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=['chat_message'], template=template\n",
        ")\n",
        "\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "llm_chain = LLMChain(prompt = prompt, llm = llm)\n",
        "\n",
        "chat_message = \"\"\"\n",
        "'janet: I am very happy to say that after many months of wanting to spend time working on bringing @ben’s vision of speech to speech machine translation in Africa to life, we now have a little bit of capacity and some strong new connections with which to take this forward.',\n",
        "'Ben: Great to see the movement here!',\n",
        "'janet: Yes!! Tee came out with us in London on wednesday as well, really glad to work with him\\nAs we have an open partnership discussion with Baaba for Jam Galaxy there is always some chance he will help us a little with the project somehow and his NGO NANN-K',\n",
        "'hruy: Great initiative with a super cool and novel objective. It can also expand to South America and parts of Asia where there is a similar problem related to languages dying and languages lacking a writing system.' \"Most of the cross-country languages like Swahili have adopted a writing system, in this case, Arabic, and the major languages usually adopt the Western alphabet. Ethiopia is the only African country with it's own writing system. Another thing is most of Africa is divided mainly into three groups: English-speaking, French-speaking, and Arabic-speaking.\",\n",
        "'janet: Thank you so much @Hruy Tsegaye for your insights and support!',\n",
        "'surgey: Hi, I suppose starting collecting/scraping/filtering speech datasets - all we could get might be a great task for Ethiopean team, it usually takes lots of time in fact, we could also delegate one of the most experienced data engineer from Russian team to master the process.',\n",
        "'janet: Thanks @sergeyshalyapin did you put this in your planned allocations shared with Jan and Rachel and if not can you please add it?'\n",
        "\"\"\"\n",
        "\n",
        "output = llm_chain.run(chat_message)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "-hty1yhE_mHO",
        "outputId": "0244a48e-aeb0-4ae2-9e88-c3361ea18168",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "-hty1yhE_mHO",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INST]\n",
            "<<SYS>>\n",
            "\n",
            "chat message: {chat_message}\n",
            "Provide concise responses based only on the given chat messages. \n",
            "Your role is to assist and answer questions within the context. \n",
            "Do not provide information beyond the chat message. \n",
            "If the context is unclear, respond with 'This topic was not discussed previously.\n",
            " \n",
            "<</SYS>>\n",
            "\n",
            " \n",
            "why does ben say he is angry?\n",
            "[/INST]\n",
            " Ben does not say he is angry in the provided chat messages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R7N3jACqA8Xe",
        "outputId": "3879fe20-3a6f-43fe-f7d3-05dcdbdd2599",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "R7N3jACqA8Xe",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Ben does not say he is angry in the provided chat messages.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}