{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YeabsiraNigusse/machine-Learning/blob/main/semantic_search/Q%26A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7ca754e0",
      "metadata": {
        "id": "7ca754e0",
        "outputId": "bdd3e61a-f7aa-45c0-fc50-cade4ac78c7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.4.10)\n",
            "Requirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic<2.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.12)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.3)\n",
            "Requirement already satisfied: fastapi<0.100.0,>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.99.1)\n",
            "Requirement already satisfied: uvicorn[standard]>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.23.2)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.5.0)\n",
            "Requirement already satisfied: pulsar-client>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.3.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.15.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.14.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.4.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.0.1)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.5)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi<0.100.0,>=0.95.2->chromadb) (0.27.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (2.0.4)\n",
            "Requirement already satisfied: huggingface_hub<0.17,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.16.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.0)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (6.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.20.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (3.7.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi<0.100.0,>=0.95.2->chromadb) (1.1.3)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install chromadb\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3dac6c41",
      "metadata": {
        "id": "3dac6c41"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "from chromadb.utils import embedding_functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2530d37a",
      "metadata": {
        "id": "2530d37a"
      },
      "outputs": [],
      "source": [
        "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
        "                api_key='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',\n",
        "                model_name=\"text-embedding-ada-002\"\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "67667623",
      "metadata": {
        "id": "67667623"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "\n",
        "client = chromadb.Client()\n",
        "\n",
        "collection = client.get_or_create_collection('demo', embedding_function=openai_ef)#collection is like database used to store documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9a7e79d7",
      "metadata": {
        "id": "9a7e79d7"
      },
      "outputs": [],
      "source": [
        "collection.add(\n",
        "    documents=['This a document about cat', 'This a document about a car'],\n",
        "    metadatas=[{'catagory': 'animal'}, {'catagory':'automobile'}],\n",
        "    ids=['id1', 'id2']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f69ee80d",
      "metadata": {
        "id": "f69ee80d",
        "outputId": "dd7a636c-2462-486e-d9b7-4cc3277d93a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': [['id2']],\n",
              " 'distances': [[0.3225654065608978]],\n",
              " 'metadatas': [[{'catagory': 'automobile'}]],\n",
              " 'embeddings': None,\n",
              " 'documents': [['This a document about a car']]}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "results = collection.query(\n",
        "    query_texts=['automobil'],\n",
        "    n_results=1\n",
        ")\n",
        "\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "47dae13a",
      "metadata": {
        "id": "47dae13a",
        "outputId": "ae3c8c38-52eb-4299-8e0f-4b4c63f940d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5161cbc0",
      "metadata": {
        "id": "5161cbc0",
        "outputId": "f352e918-e7af-4c46-ffcf-f6f61f9e7c5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file name: numpy.txt\n",
            "content: NumPy: If you need to crunch numbers\n",
            "What is it: NumPy is a Python package for working with arrays, or large collections of homogenous data. You can think of an array like a spreadsheet, where numbers are stored in columns and rows.\n",
            "\n",
            "Background: Python wasn’t originally intended for numerical computation when it was launched in 1991. Still, its ease of use caught the scientific community’s attention early on. Over the years, the open source community developed a succession of packages for numerical computing. In 2005, developer Travis Oliphant combined over a decade’s worth of open source developments into a single library for numerical computation, which he called NumPy. \n",
            "\n",
            "Features: The core feature of NumPy is support for arrays, which allows you to quickly process and manipulate large collections of data.\n",
            "\n",
            "Arrays in NumPy can be n-dimensional. This means the data can be a single column of numbers, or many columns and rows of numbers. \n",
            "NumPy has modules for performing some linear algebra functions. \n",
            "\n",
            "It also has modules for graphing and plotting numerical arrays. \n",
            "Data in NumPy arrays is homogenous, which means it must all be defined as the same type (numbers, strings, Boolean values, etc.). This means data gets processed efficiently. \n",
            "Best for: Manipulating and processing data for more advanced data science or machine learning operations. If you are crunching numbers, you need NumPy. \n",
            "\n",
            "Downsides: Because NumPy arrays are homogeneous, they are a bad fit for mixed data. You are better off using Python lists. Also, NumPy’s performance tends to drop off when working with more than 500,000 columns.\n",
            "Best place to learn: Linear Regression with NumPy and Python from Coursera.\n",
            "\n",
            "file name: pandas.txt\n",
            "content: Pandas: If you need to manipulate data\n",
            "What is it: Pandas is a package for simultaneously working with different types of labeled data. You’d use it, for example, if you need to analyze a CSV file containing numerical, alphabetical, and string data. \n",
            "\n",
            "Background: Wes McKinney released Pandas in 2008. It builds on NumPy (and, in fact, you must have NumPy installed to use Pandas) and extends that package to work with heterogeneous data. \n",
            "\n",
            "Features: The core feature of Pandas is its variety of data structures, which let users perform an assortment of analysis operations. \n",
            "\n",
            "Pandas has a variety of modules for data manipulation, including reshape, join, merge, and pivot. \n",
            "Pandas has data visualization capabilities.\n",
            "Users can perform mathematical operations including calculus and statistics without calling on outside libraries. \n",
            "It has modules that help you work around missing data.\n",
            "\n",
            "Best for: Data analysis. \n",
            "\n",
            "Downsides: Switching between vanilla Python and Pandas can be confusing, as the latter has a slightly more complex syntax. Pandas also has a steep learning curve. These factors, combined with poor documentation, can make it difficult to pick up. \n",
            "Best place to learn: Introduction to Pandas from DeepLearning.AI.\n",
            "\n",
            "\n",
            "\n",
            "file name: scikit-learn.txt\n",
            "content: Scikit-learn: If you need to do machine learning\n",
            "\n",
            "What is it: Scikit-learn is a Python library for implementing machine learning algorithms. \n",
            "\n",
            "Background: A developer named David Cournapeau originally released scikit-learn as a student in 2007. The open source community quickly adopted it and has updated it numerous times over the years. \n",
            "\n",
            "Features: The packages in Scikit-learn focus on modeling data. \n",
            "\n",
            "Scikit-learn includes every core machine learning algorithm, among them vector machines, random forests, gradient boosting, k-means clustering, and DBSCAN. \n",
            "It was designed to work seamlessly with NumPy and SciPy (both described below) for data cleaning, preparation, and calculation. \n",
            "It has modules for loading data as well as splitting it into training and test sets.\n",
            "It supports feature extraction for text and image data.\n",
            "\n",
            "Best for: Scikit-learn is a must-have for anybody working in machine learning. It is considered one of the best libraries available if you need to implement algorithms for classification, regression, clustering, model selection, and more. \n",
            "\n",
            "Downsides: Scikit-learn was built before deep learning took off. While it works great for core machine learning and data science jobs, if you are building neural nets you’ll need either TensorFlow or Pytorch (below). \n",
            "Best place to learn: Machine Learning in Python with Scikit-Learn from Data School. (Note: Scikit-learn is one of the easiest Python libraries to learn. Once you are proficient in Python itself, Scikit-learn is a snap.)\n",
            "\n",
            "file name: scipy.txt\n",
            "content: SciPy: If you need to do math for data science\n",
            "\n",
            "What is it: SciPy is a Python library for scientific computing. It contains packages and modules for performing calculations that help scientists conduct or analyze experiments. \n",
            "\n",
            "Background: In the late 1990s and early 2000s, the Python open source community began working on a collection of tools to meet the needs of the scientific community. In 2001, they released these tools as SciPy. The community remains active and is always updating and adding new features. \n",
            "\n",
            "Features: SciPy’s packages comprise a complete toolkit of mathematical techniques from calculus, linear algebra, statistics, probabilities, and more. \n",
            "\n",
            "Some of its most popular packages for data scientists are for interpolation, K-means testing, numerical integration, Fourier transforms, orthogonal distance regression, and optimization.\n",
            "SciPy also includes packages for image processing and signal processing. \n",
            "\n",
            "The Weave feature allows users to write code in C/C++ within Python. \n",
            "Best for: SciPy is a data scientist’s best friend. \n",
            "\n",
            "Downsides: Some users have found SciPy’s documentation lacking and critique several of its packages as inferior to similar packages found in MatLab. \n",
            "\n",
            "Best place to learn: SciPy Programming by Ahmad Bazzi.\n",
            "\n",
            "file name: tesorflow vs pytorch.txt\n",
            "content: If you need to do machine learning: TensorFlow vs. PyTorch\n",
            "\n",
            "TensorFlow and PyTorch perform the same essential tasks related to deep learning: They make it easy to acquire data, train models, and generate predictions. From face recognition to large language models, many neural networks are coded using either TensorFlow or PyTorch. The libraries were once markedly different, both in the front and back-end. Over time, they converged around the same set of best practices.\n",
            "\n",
            "Nonetheless, debate is ongoing within the AI community about which is best. TensorFlow, released in 2015, was the first on the scene. It dominates in commercial AI and product development, but many users complain about its complexity.\n",
            "\n",
            "PyTorch, released in 2016, is widely considered to be both easier to learn and faster to implement. It is a favorite among academics and is steadily gaining popularity in industry. However, it is known to struggle at scaling. \n",
            "\n",
            "Which to choose? \n",
            "\n",
            "TensorFlow is still the dominant deep learning library in industry. This is partly due to inertia, and partly due to the fact that TensorFlow is better than PyTorch at handling large projects and complex workflows. Its ability to handle AI products that are scaled for commercial deployment makes it a favorite for product development. \n",
            "\n",
            "If you are just jumping into deep learning and want to focus on building and prototyping models quickly, PyTorch is probably the better bet. Be aware that you may have to learn TensorFlow one day depending on your job requirements and company tech (especially if your dream job is at Google, home of TensorFlow). \n",
            "\n",
            "Learn more about the pros and cons of both libraries below.\n",
            "\n",
            "\n",
            "TensorFlow\n",
            "\n",
            "What is it? TensorFlow is an end-to-end open source library for developing, training, and deploying deep learning models. \n",
            "\n",
            "Background: TensorFlow was originally released in 2015 by Google Brain. Originally, its front end wasn’t user friendly, and it had redundant APIs that made building and implementing models cumbersome. Many of these issues have been resolved over time with updates, as well as by integrating Keras (see below) as the default front end. \n",
            "\n",
            "Features: TensorFlow has numerous packages for building deep learning models and scaling them for commercial deployment. \n",
            "\n",
            "TensorFlow users can call upon the hundreds of pre-trained models in the Dev Hub and Model Garden. The Dev Hub contains plug-and-play models while the Model Garden is intended for more advanced users who are comfortable making customizations. \n",
            "It is efficient in its use of memory, making it possible to train multiple neural networks in parallel. \n",
            "TensorFlow applications can run on a wide variety of hardware systems, including CPUs, GPUs, TPUs, and more. \n",
            "TensorFlow Lite is optimized for mobile and embedded machine learning models.\n",
            "Users can freely upload and share their machine learning experiments on Tensorboard.dev. \n",
            "Best for: Building production-ready deep learning models at scale.\n",
            "\n",
            "Downsides: Some users still complain that the front-end is fairly complicated. You may also come across critiques that TensorFlow executes slowly. This is mostly a legacy complaint from TensorFlow 1.0, when it executed operations in graph mode by default. TensorFlow 2.0 defaults to eager execution mode. \n",
            "\n",
            "Best place to learn: TensorFlow Developer Professional Certificate from DeepLearning.ai. \n",
            "\n",
            "\n",
            "Keras: \n",
            "What is it: Keras is a beginner-friendly toolkit for working with neural networks. It is the front-end interface for TensorFlow. \n",
            "\n",
            "Background: Google engineer Francois Choillet released Keras in 2015 to act as an API for a number of deep learning libraries. As of 2020, Keras is exclusive to TensorFlow.\n",
            "\n",
            "Features: Keras handles the high level tasks of building neural networks in TensorFlow, and as such contains fundamental modules like activation functions, layers, optimizers, and more. \n",
            "\n",
            "Keras supports vanilla neural networks, convolutional neural networks, and recurrent neural networks as well as utility layers including batch normalization, dropout, and pooling. \n",
            "It is designed to simplify coding deep neural networks. \n",
            "Best for: Developing deep learning networks.\n",
            "\n",
            "Downsides: It’s only available for TensorFlow users. If you use TensorFlow, you’re using Keras. \n",
            "\n",
            "Best place to learn: Introduction to Deep Learning and Neural Networks with Keras from IBM.\n",
            "\n",
            "\n",
            "PyTorch\n",
            "\n",
            "What is it: PyTorch is Facebook AI Research Lab’s answer to TensorFlow. It is an open source, general-purpose library for machine learning and data science, specifically deep learning.\n",
            "\n",
            "Background: Facebook released PyTorch in 2016 — a year after TensorFlow — and it quickly became popular with academics and other researchers who were interested in rapid prototyping. This was due to its streamlined front end and the fact that its default mode executes operations immediately (as opposed to adding them to a graph for later processing, as did TensorFlow). \n",
            "\n",
            "Features: PyTorch has many features that are analogous those in TensorFlow. Indeed, in the years since they launched, each library has been updated to include the features that users like best about the other. \n",
            "\n",
            "PyTorch has its own libraries for pre-trained models. The PyTorch Hub is aimed at academic users who want to experiment with the model design, and the Ecosystem Tools contains pre-trained models. \n",
            "PyTorch is memory-efficient and accommodates training multiple models in parallel. \n",
            "It supports a variety of hardware types.\n",
            "Best for: Rapid prototyping of deep learning models. Pytorch code runs quickly and efficiently. \n",
            "\n",
            "Downsides: Some users report that PyTorch struggles with larger projects, big datasets, and complex workflows. Developers who build AI products to be deployed at scale may prefer TensorFlow.\n",
            "\n",
            "Best place to learn:PyTorch tutorials from PyTorch.org.\n",
            "\n",
            "Conclusion\n",
            "\n",
            "The maturity of libraries for Python is one of the main reasons why it is so popular among the AI community. They make it easy to extend Python to tasks well beyond its original design. Once you have a firm grasp of the Python language and the libraries that pertain to your job, you’ll be able to build, train, and iterate on machine learning models for a wide range of applications.\n",
            "\n",
            "Even with all its libraries, however, Python doesn’t excel at everything. For instance, if you are working on AI infrastructure you might need to learn C++; If you work in finance, you will probably need to learn R. To learn more about other AI programming languages and their uses, read our guide. \n",
            "\n",
            "No matter what your AI goals are, the best thing to do is always keep learning!\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def read_files(folder_path):\n",
        "  file_data = []\n",
        "\n",
        "  for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith(\".txt\"):\n",
        "      with open(os.path.join(folder_path, file_name), 'r') as file:\n",
        "        content = file.read()\n",
        "        file_data.append({'file_name': file_name, 'content': content})\n",
        "\n",
        "  return file_data\n",
        "\n",
        "folder_path = 'drive/MyDrive/articles'\n",
        "\n",
        "datas = read_files(folder_path)\n",
        "for data in datas:\n",
        "  print(f\"file name: {data['file_name']}\")\n",
        "  print(f\"content: {data['content']}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ac89c70e",
      "metadata": {
        "id": "ac89c70e",
        "outputId": "02b50743-2f57-4d4c-8017-1553dddaefaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'source': 'numpy.txt'},\n",
              " {'source': 'pandas.txt'},\n",
              " {'source': 'scikit-learn.txt'},\n",
              " {'source': 'scipy.txt'},\n",
              " {'source': 'tesorflow vs pytorch.txt'}]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "documents = []\n",
        "metadatas = []\n",
        "ids = []\n",
        "\n",
        "for index, data in enumerate(datas):\n",
        "  documents.append(data['content'])\n",
        "  metadatas.append({'source':data['file_name']})\n",
        "  ids.append(str(index+1))\n",
        "\n",
        "metadatas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "50ec7455",
      "metadata": {
        "id": "50ec7455"
      },
      "outputs": [],
      "source": [
        "client = chromadb.PersistentClient(path=\"database\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "8dc99a3c",
      "metadata": {
        "id": "8dc99a3c"
      },
      "outputs": [],
      "source": [
        "lib_collection = client.get_or_create_collection('lib_collection',embedding_function=openai_ef)\n",
        "\n",
        "lib_collection.add(\n",
        "    documents=documents,\n",
        "    metadatas=metadatas,\n",
        "    ids=ids\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f415908e",
      "metadata": {
        "id": "f415908e",
        "outputId": "84405b4b-50a3-41a2-c2fe-3c23e376b6a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Scikit-learn: If you need to do machine learning\\n\\nWhat is it: Scikit-learn is a Python library for implementing machine learning algorithms. \\n\\nBackground: A developer named David Cournapeau originally released scikit-learn as a student in 2007. The open source community quickly adopted it and has updated it numerous times over the years. \\n\\nFeatures: The packages in Scikit-learn focus on modeling data. \\n\\nScikit-learn includes every core machine learning algorithm, among them vector machines, random forests, gradient boosting, k-means clustering, and DBSCAN. \\nIt was designed to work seamlessly with NumPy and SciPy (both described below) for data cleaning, preparation, and calculation. \\nIt has modules for loading data as well as splitting it into training and test sets.\\nIt supports feature extraction for text and image data.\\n\\nBest for: Scikit-learn is a must-have for anybody working in machine learning. It is considered one of the best libraries available if you need to implement algorithms for classification, regression, clustering, model selection, and more. \\n\\nDownsides: Scikit-learn was built before deep learning took off. While it works great for core machine learning and data science jobs, if you are building neural nets you’ll need either TensorFlow or Pytorch (below). \\nBest place to learn: Machine Learning in Python with Scikit-Learn from Data School. (Note: Scikit-learn is one of the easiest Python libraries to learn. Once you are proficient in Python itself, Scikit-learn is a snap.)',\n",
              "  'SciPy: If you need to do math for data science\\n\\nWhat is it: SciPy is a Python library for scientific computing. It contains packages and modules for performing calculations that help scientists conduct or analyze experiments. \\n\\nBackground: In the late 1990s and early 2000s, the Python open source community began working on a collection of tools to meet the needs of the scientific community. In 2001, they released these tools as SciPy. The community remains active and is always updating and adding new features. \\n\\nFeatures: SciPy’s packages comprise a complete toolkit of mathematical techniques from calculus, linear algebra, statistics, probabilities, and more. \\n\\nSome of its most popular packages for data scientists are for interpolation, K-means testing, numerical integration, Fourier transforms, orthogonal distance regression, and optimization.\\nSciPy also includes packages for image processing and signal processing. \\n\\nThe Weave feature allows users to write code in C/C++ within Python. \\nBest for: SciPy is a data scientist’s best friend. \\n\\nDownsides: Some users have found SciPy’s documentation lacking and critique several of its packages as inferior to similar packages found in MatLab. \\n\\nBest place to learn: SciPy Programming by Ahmad Bazzi.']]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "results = lib_collection.query(\n",
        "    query_texts=['what is scikit-learn'],\n",
        "    n_results=2\n",
        ")\n",
        "\n",
        "results['documents']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "6082128a",
      "metadata": {
        "id": "6082128a"
      },
      "outputs": [],
      "source": [
        "res = \"\\n\".join(str(item) for item in results['documents'])\n",
        "\n",
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]\n",
        "\n",
        "prompt=f\"\"\"\n",
        "{res}\n",
        " \\n\\n Based on the data in ```,\n",
        "\n",
        "Perform the following actions:\n",
        "\n",
        "Name: <name of the librery>\n",
        "downside: <downside of the librery>\n",
        "best place to learn: <best place to learn the librery>\n",
        "features: <features of the librery>\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_completion(prompt)"
      ],
      "metadata": {
        "id": "_XlQiWPs2HeD",
        "outputId": "bde751c8-7589-4333-d8d5-8e92cbfe4b1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "id": "_XlQiWPs2HeD",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Name: Scikit-learn\\nDownside: Scikit-learn was built before deep learning took off, so it may not be the best choice for building neural nets.\\nBest place to learn: Machine Learning in Python with Scikit-Learn from Data School.\\nFeatures: Scikit-learn includes every core machine learning algorithm, supports feature extraction for text and image data, and has modules for loading data and splitting it into training and test sets. It is considered one of the best libraries available for implementing algorithms for classification, regression, clustering, model selection, and more.\\n\\nName: SciPy\\nDownside: Some users have found SciPy's documentation lacking and critique several of its packages as inferior to similar packages found in MatLab.\\nBest place to learn: SciPy Programming by Ahmad Bazzi.\\nFeatures: SciPy's packages comprise a complete toolkit of mathematical techniques, including calculus, linear algebra, statistics, probabilities, interpolation, K-means testing, numerical integration, Fourier transforms, orthogonal distance regression, optimization, image processing, signal processing, and the ability to write code in C/C++ within Python. It is a valuable tool for data scientists.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}